{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lowva-master.ipynb","provenance":[{"file_id":"1HFLof-3ziCVsataA8H0aHlqabG6khqUg","timestamp":1606159001708},{"file_id":"1MeL_-joHg84EQHTLiIUHww-OSupBm_uS","timestamp":1594964075186}],"collapsed_sections":["yZE7-6OPyopx","VEwYhFCYzQdJ","XUZhY0tX1qe5","fzIj1773cEvK","knWFjuOsJL8O","mzEykDkYRnOL"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yZE7-6OPyopx"},"source":["# Google Colab Preamble\n","Imports necessary libraries and performs basic GPU and memory checks on Colab instance. Ends with mounting Google Drive for use as datastore"]},{"cell_type":"code","metadata":{"id":"1dagMyb7Q4S_"},"source":["#change your dependencies as you see fit\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import glob \n","import matplotlib.pyplot as plt\n","import h5py \n","import pandas as pd\n","import random \n","from sklearn.metrics import classification_report\n","\n","%matplotlib inline\n","\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","tf.keras.backend.set_image_data_format('channels_last')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDlRJA1pQ7Yt"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1KbsW8GQ-ok"},"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n","  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n","  print('re-execute this cell.')\n","else:\n","  print('You are using a high-RAM runtime!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nBVmhM8RBfe"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEwYhFCYzQdJ"},"source":["# Copying Data\n","Loads lightly processed data from mounted Google Drive. ```lowva-extrinsicdata.csv``` is a CSV containing patient ID, outcome of low VA progression, and unstructuredw EMR text. ```lowva-structured-02.csv``` is a CSV containing patient ID, outcome of low VA progression, and all structured data fields from EMR (ask Sophia re: definitions). Note: first row of ```lowva-structured-02.csv``` are headers for structured fields (```lowva-extrinsicdata.csv``` does not have such a row).\n","\n","```pubmed_cbow_vocabulary.txt``` and ```pubmed_cbow_embeddings.h5``` hold the information necessary for the 300-dimensional CBOW trained embeddings on Pubmed Abstracts pertaining to Ophthalmology. ```pubmed_cbow_vocabulary.txt``` is the vocabulary list (listed in order of index they appear in and include the end-padding used by CBOW for the end / beginning of a line). ```pubmed_cbow_embeddings.h5``` is the HDF5 format saved Tensorflow model corresponding to the CBOW training. The embeddings layer can be accessed following model load at index 1 (not 0)."]},{"cell_type":"code","metadata":{"id":"_5qhGGUMMng7"},"source":["import shutil \n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/data/lowva/lowva-extrinsicdata.csv\", \"lowva-extrinsicdata.csv\")\n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/data/lowva/lowva-structured-02.csv\", \"lowva-structured-02.csv\")\n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/data/pubmed_cbow_vocabulary.txt\", \"pubmed_cbow_vocabulary.txt\")\n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/data/pubmed_cbow_embeddings.h5\", \"pubmed_cbow_embeddings.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XUZhY0tX1qe5"},"source":["# Transformer\n","Code defining Transformer layer, borrowed from [Keras examples: \"Text classification with Transformer\" tutorial](https://keras.io/examples/nlp/text_classification_with_transformer/). Main modification is ```TokenAndPositionPreTrainedEmbedding``` layer which was created to support pre-trained embeddings (i.e. the Pubmed embeddings)"]},{"cell_type":"code","metadata":{"id":"t5e-xz9brIj7"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8, **kwargs):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = layers.Dense(embed_dim)\n","        self.key_dense = layers.Dense(embed_dim)\n","        self.value_dense = layers.Dense(embed_dim)\n","        self.combine_heads = layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        # x.shape = [batch_size, seq_len, embedding_dim]\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        query = self.separate_heads(\n","            query, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        key = self.separate_heads(\n","            key, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        value = self.separate_heads(\n","            value, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(\n","            attention, perm=[0, 2, 1, 3]\n","        )  # (batch_size, seq_len, num_heads, projection_dim)\n","        concat_attention = tf.reshape(\n","            attention, (batch_size, -1, self.embed_dim)\n","        )  # (batch_size, seq_len, embed_dim)\n","        output = self.combine_heads(\n","            concat_attention\n","        )  # (batch_size, seq_len, embed_dim)\n","        return output\n","\n","    def get_config(self):\n","        config = super(MultiHeadSelfAttention, self).get_config()\n","        config.update({'embed_dim': self.embed_dim, 'num_heads': self.num_heads})\n","        return config\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","        self.ff_dim = ff_dim\n","        self.embed_dim = embed_dim \n","        self.num_heads = num_heads\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","    def get_config(self):\n","        config = super(TransformerBlock, self).get_config()\n","        config.update({'embed_dim': self.embed_dim, 'num_heads': self.num_heads, 'ff_dim': self.ff_dim})\n","        return config\n","\n","class TokenAndPositionPreTrainedEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim, pretrained_emb, trainable=False, **kwargs):\n","        super(TokenAndPositionPreTrainedEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, embeddings_initializer = tf.keras.initializers.Constant(pretrained_emb), trainable=trainable)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.embed_dim = embed_dim\n","        self.maxlen = maxlen\n","        self.vocab_size = vocab_size \n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","    def get_config(self):\n","        config = super(TokenAndPositionPreTrainedEmbedding, self).get_config()\n","        config.update({'embed_dim': self.embed_dim, 'maxlen': self.maxlen, 'vocab_size': self.vocab_size})\n","        return config\n","\n","class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","        self.embed_dim = embed_dim\n","        self.maxlen = maxlen\n","        self.vocab_size = vocab_size \n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n","\n","    def get_config(self):\n","        config = super(TokenAndPositionEmbedding, self).get_config()\n","        config.update({'embed_dim': self.embed_dim, 'maxlen': self.maxlen, 'vocab_size': self.vocab_size})\n","        return config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzIj1773cEvK"},"source":["# Loading Data\n","The next block loads the data from the CSVs into three separate numpy arrays (```tokenArray``` for unstructured data, ```structuredArray``` for structured data, and ```outputArray``` for all the output values) and uses the patient ID to insure that they are all in the exact same order (i.e. ```tokenArray[100]``` is the same patient as ```structuredArray[100]``` and had an outcome of ```outputArray[100]```)\n","\n","This block also loads the Pubmed embeddings into ```embedding_matrix``` and the corresponding vocabulary is loaded into the tokenizer which is used to tokenize the unstructured text which feeds into ```tokenArray``` (for consistency uses the same tokenizer used in the embedding training, a now-deprecated tokenizer from the library ```tensorflow_datasets``` called ```TokenTextEncoder``` which takes an ordered vocabulary list as an argument)"]},{"cell_type":"code","metadata":{"id":"0FrrKyUSNM4M"},"source":["# code to load the pre-trained embeddings and dataset\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import csv\n","\n","def isfloat(value):\n","    try:\n","        float(value)\n","        return True\n","    except ValueError:\n","        return False\n","\n","with open('pubmed_cbow_vocabulary.txt', 'r') as f:\n","    vocabulary = []\n","    for row in f:\n","        vocabulary.append(row.strip())\n","\n","vocabulary_size = len(vocabulary) + 2\n","tokenizer = tfds.deprecated.text.Tokenizer()\n","\n","width = 1000\n","embedding_dimension = 300\n","encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary, tokenizer=tokenizer)\n","csv.field_size_limit(1310720)\n","print('\\nloading structured data')\n","with open('lowva-structured-02.csv', 'r') as f:\n","    r = csv.reader(f)\n","    i = 0\n","    structuredDict = {}\n","    for row in r:\n","        i += 1\n","        if i == 1:\n","            continue\n","        else:\n","            key = row[0].strip()\n","            values = np.array([float(i) for i in row[2:] if isfloat(i)])\n","            structuredDict[key] = values\n","\n","print('loading and joining with unstructured data')\n","with open('lowva-extrinsicdata.csv', 'r') as f:\n","    r = csv.reader(f)\n","    i = 0\n","    tokenArray = []\n","    outputArray = []\n","    structuredArray = []\n","    for row in r:\n","        i += 1\n","        if row[0].strip() in structuredDict:\n","            structvalues = structuredDict[row[0].strip()]\n","            output = np.array([int(row[1])])\n","            tokens = encoder.encode(row[2])\n","            tokens = tokens[0:width]\n","            if len(tokens) < width:\n","                tokens = tokens + [0 for i in range(width-len(tokens))]\n","\n","            structuredArray.append(structvalues)\n","            outputArray.append(output)\n","            tokenArray.append(tokens)\n","\n","del structuredDict\n","tokenArray = np.array(tokenArray)\n","outputArray = np.array(outputArray)\n","structuredArray = np.array(structuredArray)\n","\n","print(tokenArray.shape, structuredArray.shape, outputArray.shape)\n","\n","print('loading Pubmed EMR vectors')\n","model = tf.keras.models.load_model('pubmed_cbow_embeddings.h5')\n","embedding_matrix = np.zeros((vocabulary_size, embedding_dimension))\n","e = model.layers[1]\n","embedding_matrix = e.get_weights()[0]\n","print(embedding_matrix.shape)\n","del model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"knWFjuOsJL8O"},"source":["# The Models\n","The rest of this notebook is highly repetitive and lists every example / corresponding data that was run. The basic structure includes:\n","\n","*   Using ```tf.data``` to generate the correct data pipelines to feed the model\n","*   Setting up the model in Keras, training & evaluating the model, and then saving the weights for later\n","*   Loading the best performance (by AUROC) model and then using the validation set (called ```validation_dataset```, also used in model training for early-stopping) to determine the decision threshold with the best F1 score\n","*   Evaluating the best models (based on the validation set) on the holdout set (called ```test_dataset```)\n","\n","Will provide detailed commentary for the first model (1) Structured and will then just have blocks of code afterwards. This is structured so you can just keep running through and it shouldn't break anything"]},{"cell_type":"markdown","metadata":{"id":"oPuyqAtzMUqG"},"source":["## (1) Structured --> 1024-D-64-1\n","### Using ```tf.data``` to prep datasets for model\n","The code uses Tensorflow's ```tf.data``` paradigm to load data into Keras. Because this code is used for multiple configurations of model, the code comments out the data pipelines that are not relevant (in this case, because the first model is Structured data only, the input datasets that include the unstructured data are unnecessary). \n","\n","Afterwards, the dataset is loaded from memory and cut up\n","*    The first 300 items are used as holdout set\n","*    The second 300 items are used as a validation set for early-stopping and interim analysis of overfitting\n","*    The remainder of the data is used for training"]},{"cell_type":"code","metadata":{"id":"zuLZRWWVPuvN"},"source":["total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ITNNSneNJRZ"},"source":["### Keras model setup, training, and evaluation\n","The Keras models are described by the name of the section. For Structured models, simple feed-forward fully-connected layers are used, generally with Dropout and regularization.  For the Unstructured data, architectures range from those derived from the TextCNN model ([link to original paper](https://arxiv.org/abs/1408.5882)) to Transformers. Combination models also experiment with different time points at which the two separate models are merged. \n","\n","Due to the imbalance, training reports more than just accuracy, but also true positives, true negatives, false positives, false negatives, recall, precision, and AUROC (using Tensorflow's native metrics capabilities). Training is also set to early stop based on whether or not validation dataset losses continue to decline (with a patience of 4 epochs) and a 10x reduction in learning rate if two subsequent epochs do not show improvement. \n","\n","All model weights are saved so they can be revisited, and the files are manually renamed to include the validation set AUROC in the title. Note: these model weight files are distinct from those used  for the Pubmed embeddings and uses Tensorflow's ```save_weights``` approach which is more lightweight but requires additional files -- see [Tensorflow web site](https://www.tensorflow.org/tutorials/keras/save_and_load#manually_save_weights). \n","\n","All models are trained at least 3 times with the model with the best validation set performance saved for later evaluation (see \"Loading Model Weights to Evaluate on Holdout\" below). For simplicity, the results of the runs including the holdout dataset evaluation (which uses decision threshold tuning to maximize F1) are included here"]},{"cell_type":"code","metadata":{"id":"7IGtKnntQlo7"},"source":["input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(sl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('struct01_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AjeC0oMJV-tX"},"source":["Results:\n","*    (Adam = 0.0001) loss: 0.5863 - tp: 75.0000 - fp: 34.0000 - tn: 142.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.6881 - auroc: 0.8104 - acc: 0.7233 \n","    *    Holdout: AUROC: 78.53566408157349,\n","Sensitivity/Recall: 77.2357702255249, Precision: 55.55555820465088, F1: 64.6258513860928\n","*   (Adam = 0.0001) loss: 0.5925 - tp: 76.0000 - fp: 35.0000 - tn: 141.0000 - fn: 48.0000 - sen: 0.6129 - prc: 0.6847 - auroc: 0.8095 - acc: 0.7233\n","*    (Adam = 0.0001) loss: 0.5934 - tp: 76.0000 - fp: 34.0000 - tn: 142.0000 - fn: 48.0000 - sen: 0.6129 - prc: 0.6909 - auroc: 0.8076 - acc: 0.7267 "]},{"cell_type":"markdown","metadata":{"id":"pXB7KCsUXeaw"},"source":["## (2) structured --> 64-D-64-1- ARVO abstract 2021"]},{"cell_type":"code","metadata":{"id":"Wg5GGIlqXqUq"},"source":["total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i__Z3BJ9Xrqu"},"source":["input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(sl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('struct02_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULgL8pUGX_cP"},"source":["Results\n","* (Adam = 0.0001) loss: 0.5784 - tp: 73.0000 - fp: 31.0000 - tn: 145.0000 - fn: 51.0000 - sen: 0.5887 - prc: 0.7019 - auroc: 0.8104 - acc: 0.7267\n","* (Adam = 0.0001) loss: 0.5735 - tp: 75.0000 - fp: 34.0000 - tn: 142.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.6881 - auroc: 0.8143 - acc: 0.7233\n","    * Holdout: AUROC: 79.0661871433258, Sensitivity/Recall: 74.79674816131592, Precision: 58.598726987838745, F1: 65.71428633815779\n","* (Adam = 0.0001) loss: 0.5741 - tp: 73.0000 - fp: 30.0000 - tn: 146.0000 - fn: 51.0000 - sen: 0.5887 - prc: 0.7087 - auroc: 0.8141 - acc: 0.7300 \n","* (Adam = 0.0001) loss: 0.5786 - tp: 74.0000 - fp: 33.0000 - tn: 143.0000 - fn: 50.0000 - sen: 0.5968 - prc: 0.6916 - auroc: 0.8099 - acc: 0.7233 "]},{"cell_type":"markdown","metadata":{"id":"kZkWFN6aYXZl"},"source":["## (3) structured --> elasticnet"]},{"cell_type":"code","metadata":{"id":"FoEfUd3PYbv0"},"source":["total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IG9xQ-AWYeYB"},"source":["input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","output = tf.keras.layers.Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l1_l2())(input2)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('struct03_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qgzl4nkPYm9l"},"source":["Results (b/c of poor performance did not run test set or save model):\n","* (Adam = 0.001) loss: 0.6061 - tp: 49.0000 - fp: 31.0000 - tn: 145.0000 - fn: 75.0000 - sen: 0.3952 - prc: 0.6125 - auroc: 0.7481 - acc: 0.6467\n","* (Adam = 0.001) loss: 0.6056 - tp: 48.0000 - fp: 30.0000 - tn: 146.0000 - fn: 76.0000 - sen: 0.3871 - prc: 0.6154 - auroc: 0.7473 - acc: 0.6467\n","* (Adam = 0.001) loss: 0.6058 - tp: 49.0000 - fp: 31.0000 - tn: 145.0000 - fn: 75.0000 - sen: 0.3952 - prc: 0.6125 - auroc: 0.7471 - acc: 0.6467\n","* (Adam = 0.001) loss: 0.6056 - tp: 48.0000 - fp: 29.0000 - tn: 147.0000 - fn: 76.0000 - sen: 0.3871 - prc: 0.6234 - auroc: 0.7479 - acc: 0.6500"]},{"cell_type":"markdown","metadata":{"id":"gByyMOQLY456"},"source":["## (4) Paper TextCNN"]},{"cell_type":"code","metadata":{"id":"g6YhrbPYY-Cg"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NIgka4NZCnS"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_unstruct04_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bl7V7PpEZYaH"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5486 - tp: 78.0000 - fp: 33.0000 - tn: 143.0000 - fn: 46.0000 - sen: 0.6290 - prc: 0.7027 - auroc: 0.8067 - acc: 0.7367\n","* (Adam = 0.0001) loss: 0.5358 - tp: 88.0000 - fp: 35.0000 - tn: 141.0000 - fn: 36.0000 - sen: 0.7097 - prc: 0.7154 - auroc: 0.8229 - acc: 0.7633\n","    * Holdout: AUROC: 80.83229660987854, Sensitivity/Recall: 69.10569071769714, Precision: 66.92913174629211, F1: 67.99999874572752\n","* (Adam = 0.0001) loss: 0.5532 - tp: 81.0000 - fp: 33.0000 - tn: 143.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.7105 - auroc: 0.8073 - acc: 0.7467 \n","* (Adam = 0.0001) loss: 0.5437 - tp: 81.0000 - fp: 34.0000 - tn: 142.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.7043 - auroc: 0.8144 - acc: 0.7433 "]},{"cell_type":"markdown","metadata":{"id":"uYGGMVHfueRY"},"source":["## (5) TextCNN + 64-D-64 + 64-1"]},{"cell_type":"code","metadata":{"id":"iVQSMCIUun-Z"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPUwKEMaus11"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","combo = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(combo)\n","nl = combo\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo05_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSGcByqjvBBZ"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5927 - tp: 86.0000 - fp: 39.0000 - tn: 137.0000 - fn: 38.0000 - sen: 0.6935 - prc: 0.6880 - auroc: 0.8204 - acc: 0.7433 \n","* (Adam = 0.0001) loss: 0.5773 - tp: 83.0000 - fp: 33.0000 - tn: 143.0000 - fn: 41.0000 - sen: 0.6694 - prc: 0.7155 - auroc: 0.8289 - acc: 0.7533\n","    * Holdout: AUROC: 80.15249967575073,\n","Sensitivity/Recall: 82.11382031440735, Precision: 56.111109256744385, F1: 66.66666508632457\n","* (Adam = 0.0001) loss: 0.5865 - tp: 82.0000 - fp: 36.0000 - tn: 140.0000 - fn: 42.0000 - sen: 0.6613 - prc: 0.6949 - auroc: 0.8197 - acc: 0.7400\n","* (Adam = 0.0001) loss: 0.5839 - tp: 86.0000 - fp: 34.0000 - tn: 142.0000 - fn: 38.0000 - sen: 0.6935 - prc: 0.7167 - auroc: 0.8262 - acc: 0.7600 "]},{"cell_type":"markdown","metadata":{"id":"3FSM82BevSry"},"source":["## (6) TextCNN-end + 64-D-64-end + 1"]},{"cell_type":"code","metadata":{"id":"EMORROvovWZB"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LklY0WigvYYP"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dense(1, activation='relu')(nl)\n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","sl = tf.keras.layers.Dense(1, activation='relu')(sl)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo06_lowva_weights-XXXX')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a35xuNBQvgTk"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5948 - tp: 81.0000 - fp: 35.0000 - tn: 141.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.6983 - auroc: 0.7895 - acc: 0.7400\n","* (Adam = 0.0001) loss: 0.5998 - tp: 77.0000 - fp: 38.0000 - tn: 138.0000 - fn: 47.0000 - sen: 0.6210 - prc: 0.6696 - auroc: 0.7895 - acc: 0.7167 \n","* (Adam = 0.0001) loss: 0.5945 - tp: 70.0000 - fp: 31.0000 - tn: 145.0000 - fn: 54.0000 - sen: 0.5645 - prc: 0.6931 - auroc: 0.8091 - acc: 0.7167 "]},{"cell_type":"markdown","metadata":{"id":"KwIPkotbvtEO"},"source":["## (7) TextCNN-1024 + struct + 64-D-64-1"]},{"cell_type":"code","metadata":{"id":"4nGOGf5lvxsX"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IJWB6k38v0G1"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = input2\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo07_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfsB4oJSv7OZ"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5859 - tp: 78.0000 - fp: 38.0000 - tn: 138.0000 - fn: 46.0000 - sen: 0.6290 - prc: 0.6724 - auroc: 0.7962 - acc: 0.7200 \n","* (Adam = 0.0001) loss: 0.5964 - tp: 93.0000 - fp: 59.0000 - tn: 117.0000 - fn: 31.0000 - sen: 0.7500 - prc: 0.6118 - auroc: 0.8019 - acc: 0.7000 \n","* (Adam = 0.0001) loss: 0.5627 - tp: 84.0000 - fp: 47.0000 - tn: 129.0000 - fn: 40.0000 - sen: 0.6774 - prc: 0.6412 - auroc: 0.8085 - acc: 0.7100 \n","* (Adam = 0.0001) loss: 0.5858 - tp: 92.0000 - fp: 55.0000 - tn: 121.0000 - fn: 32.0000 - sen: 0.7419 - prc: 0.6259 - auroc: 0.8091 - acc: 0.7100 \n","    * Holdout: AUROC: 80.07441759109497,\n","Sensitivity/Recall: 82.92682766914368, Precision: 60.00000238418579, F1: 69.62457442042287"]},{"cell_type":"markdown","metadata":{"id":"__oDKOzrwFaK"},"source":["## (8) (6) with sigmoids"]},{"cell_type":"code","metadata":{"id":"QXjnlJZKwH1_"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcof7gP6wJ1s"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","sl = tf.keras.layers.Dense(1, activation='sigmoid')(sl)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo08_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPygU0BcwPdU"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.6270 - tp: 54.0000 - fp: 17.0000 - tn: 159.0000 - fn: 70.0000 - sen: 0.4355 - prc: 0.7606 - auroc: 0.7622 - acc: 0.7100 \n","* (Adam = 0.0001) loss: 0.6416 - tp: 81.0000 - fp: 63.0000 - tn: 113.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.5625 - auroc: 0.7014 - acc: 0.6467 "]},{"cell_type":"markdown","metadata":{"id":"JqiDUYk9wUeh"},"source":["## (9) late concatenate TextCNN output with full structured vector"]},{"cell_type":"code","metadata":{"id":"aCvzy-wPwXgx"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0SKf-_wGwZm2"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dense(1, activation='relu')(nl)\n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = input2\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo09_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0N38E-RlwfKd"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5564 - tp: 83.0000 - fp: 39.0000 - tn: 137.0000 - fn: 41.0000 - sen: 0.6694 - prc: 0.6803 - auroc: 0.8125 - acc: 0.7333 \n","* (Adam = 0.0001) loss: 0.5542 - tp: 81.0000 - fp: 40.0000 - tn: 136.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.6694 - auroc: 0.8133 - acc: 0.7233\n","    * Holdout: AUROC: 78.93757224082947,\n","Sensitivity/Recall: 71.54471278190613, Precision: 64.23357725143433, F1: 67.69230683756294\n","* (Adam = 0.0001) loss: 0.5562 - tp: 81.0000 - fp: 40.0000 - tn: 136.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.6694 - auroc: 0.8088 - acc: 0.7233 \n","* (Adam = 0.0001) loss: 0.5576 - tp: 78.0000 - fp: 32.0000 - tn: 144.0000 - fn: 46.0000 - sen: 0.6290 - prc: 0.7091 - auroc: 0.8054 - acc: 0.7400"]},{"cell_type":"markdown","metadata":{"id":"Iqgj8CQEwpz8"},"source":["## (10) late concatenate structured output with first flat TextCNN layer"]},{"cell_type":"code","metadata":{"id":"MuSjKbSywsq8"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTkOzroJwvEB"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1)\n","nl = tf.keras.layers.Dense(512, activation='relu')(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","sl = tf.keras.layers.Dropout(0.50)(sl)\n","sl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(sl)\n","sl = tf.keras.layers.Dense(1, activation='relu')(sl)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo10_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5dKY5j6Jw2IH"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5663 - tp: 74.0000 - fp: 36.0000 - tn: 140.0000 - fn: 50.0000 - sen: 0.5968 - prc: 0.6727 - auroc: 0.8100 - acc: 0.7133\n","    * Holdout: AUROC: 80.85755705833435,\n","Sensitivity/Recall: 79.67479825019836, Precision: 62.42038011550903, F1: 69.99999929067427\n","* (Adam = 0.0001) loss: 0.5688 - tp: 88.0000 - fp: 45.0000 - tn: 131.0000 - fn: 36.0000 - sen: 0.7097 - prc: 0.6617 - auroc: 0.8008 - acc: 0.7300 \n","* (Adam = 0.0001) loss: 0.5619 - tp: 83.0000 - fp: 43.0000 - tn: 133.0000 - fn: 41.0000 - sen: 0.6694 - prc: 0.6587 - auroc: 0.8087 - acc: 0.7200 \n","* (Adam = 0.0001) loss: 0.5562 - tp: 84.0000 - fp: 43.0000 - tn: 133.0000 - fn: 40.0000 - sen: 0.6774 - prc: 0.6614 - auroc: 0.8093 - acc: 0.7233 "]},{"cell_type":"markdown","metadata":{"id":"HgMSX1zkxFlN"},"source":["## (11) TextCNN-1024 + struct -64 + 64-D-64-1"]},{"cell_type":"code","metadata":{"id":"LrrVsLmfxIIS"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t38G6TcHxKRI"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(64, activation='relu',\n","kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo11_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"psMmPwxlxSgI"},"source":["Result:\n","* (Adam = 0.0001) loss: 0.5823 - tp: 85.0000 - fp: 53.0000 - tn: 123.0000 - fn: 39.0000 - sen: 0.6855 - prc: 0.6159 - auroc: 0.7971 - acc: 0.6933 \n","* (Adam = 0.0001) loss: 0.5971 - tp: 94.0000 - fp: 64.0000 - tn: 112.0000 - fn: 30.0000 - sen: 0.7581 - prc: 0.5949 - auroc: 0.7895 - acc: 0.6867 \n","* (Adam = 0.0001) loss: 0.5730 - tp: 95.0000 - fp: 51.0000 - tn: 125.0000 - fn: 29.0000 - sen: 0.7661 - prc: 0.6507 - auroc: 0.8131 - acc: 0.7333 \n","    * Holdout: AUROC: 80.58196902275085,\n","Sensitivity/Recall: 77.2357702255249, Precision: 62.5, F1: 69.09090823780406\n","* (Adam = 0.0001) loss: 0.5747 - tp: 95.0000 - fp: 55.0000 - tn: 121.0000 - fn: 29.0000 - sen: 0.7661 - prc: 0.6333 - auroc: 0.8018 - acc: 0.7200 "]},{"cell_type":"markdown","metadata":{"id":"qVjX8NGnxdSI"},"source":["## (12) (11) but with struct-256 + 256-D-64-1"]},{"cell_type":"code","metadata":{"id":"Dnp2tZgfxfym"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5DP8fjoxiK3"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(256, activation='relu',\n","kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo12_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bqq0KJSnxojf"},"source":["Result:\n","* (Adam = 0.0001) loss: 0.5921 - tp: 91.0000 - fp: 45.0000 - tn: 131.0000 - fn: 33.0000 - sen: 0.7339 - prc: 0.6691 - auroc: 0.8329 - acc: 0.7400 \n","    * Holdout: AUROC: 81.61315321922302,\n","Sensitivity/Recall: 85.36585569381714, Precision: 57.065218687057495, F1: 68.40391037911867\n","* (Adam = 0.0001) loss: 0.5960 - tp: 100.0000 - fp: 53.0000 - tn: 123.0000 - fn: 24.0000 - sen: 0.8065 - prc: 0.6536 - auroc: 0.8324 - acc: 0.7433 \n","* (Adam = 0.0001) loss: 0.5780 - tp: 84.0000 - fp: 46.0000 - tn: 130.0000 - fn: 40.0000 - sen: 0.6774 - prc: 0.6462 - auroc: 0.8256 - acc: 0.7133 \n","* (Adam = 0.0001) loss: 0.5804 - tp: 97.0000 - fp: 54.0000 - tn: 122.0000 - fn: 27.0000 - sen: 0.7823 - prc: 0.6424 - auroc: 0.8328 - acc: 0.7300"]},{"cell_type":"markdown","metadata":{"id":"xIrYHZLvxzrd"},"source":["## (13) (11) but with struct-256 + 256-D-256-D-64-1"]},{"cell_type":"code","metadata":{"id":"ArzjOrQQx2l3"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xRfwjY7x4r5"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],)) \n","sl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","combo = tf.keras.layers.Concatenate()([nl, sl]) \n","nl = combo \n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo13_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QHXVc4pWx9zI"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5718 - tp: 84.0000 - fp: 42.0000 - tn: 134.0000 - fn: 40.0000 - sen: 0.6774 - prc: 0.6667 - auroc: 0.8281 - acc: 0.7267\n","    * Holdout: AUROC: 81.60167336463928,\n","Sensitivity/Recall: 82.11382031440735, Precision: 60.8433723449707, F1: 69.89619271499818\n","* (Adam = 0.0001) loss: 0.6198 - tp: 95.0000 - fp: 62.0000 - tn: 114.0000 - fn: 29.0000 - sen: 0.7661 - prc: 0.6051 - auroc: 0.8012 - acc: 0.6967\n","* (Adam = 0.0001) loss: 0.5982 - tp: 101.0000 - fp: 61.0000 - tn: 115.0000 - fn: 23.0000 - sen: 0.8145 - prc: 0.6235 - auroc: 0.8150 - acc: 0.7200 \n","* (Adam = 0.0001) loss: 0.5852 - tp: 85.0000 - fp: 47.0000 - tn: 129.0000 - fn: 39.0000 - sen: 0.6855 - prc: 0.6439 - auroc: 0.8126 - acc: 0.7133 "]},{"cell_type":"markdown","metadata":{"id":"3EeZZPRvyIx2"},"source":["## (14) (13) but with 256-D-256-64-1"]},{"cell_type":"code","metadata":{"id":"B4qmcV0JyK3I"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MK-f7WuEyONk"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo14_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkJ0o273ygBx"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5924 - tp: 96.0000 - fp: 50.0000 - tn: 126.0000 - fn: 28.0000 - sen: 0.7742 - prc: 0.6575 - auroc: 0.8249 - acc: 0.7400 \n","* (Adam = 0.0001) loss: 0.5926 - tp: 85.0000 - fp: 41.0000 - tn: 135.0000 - fn: 39.0000 - sen: 0.6855 - prc: 0.6746 - auroc: 0.8238 - acc: 0.7333 \n","* (Adam = 0.0001) loss: 0.5862 - tp: 88.0000 - fp: 44.0000 - tn: 132.0000 - fn: 36.0000 - sen: 0.7097 - prc: 0.6667 - auroc: 0.8232 - acc: 0.7333 \n","* (Adam = 0.0001) loss: 0.6070 - tp: 102.0000 - fp: 61.0000 - tn: 115.0000 - fn: 22.0000 - sen: 0.8226 - prc: 0.6258 - auroc: 0.8252 - acc: 0.7233\n","    * Holdout: AUROC: 80.96779584884644,\n","Sensitivity/Recall: 79.67479825019836, Precision: 62.82051205635071, F1: 70.25089616347687"]},{"cell_type":"markdown","metadata":{"id":"AxE11x_5yqdV"},"source":["## (15) (12) but with struct-1024 - arvo abstract 2021"]},{"cell_type":"code","metadata":{"id":"zT3DpkpCysaI"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Mqm_4GHyuXT"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix), input_length = width, trainable=False)(input1) \n","nl = tf.keras.layers.Dense(512, activation='relu')(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo15_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yaKn2r2Ny1Re"},"source":["* (Adam = 0.0001) loss: 0.5789 - tp: 96.0000 - fp: 53.0000 - tn: 123.0000 - fn: 28.0000 - sen: 0.7742 - prc: 0.6443 - auroc: 0.8336 - acc: 0.7300\n","    * Holdout: AUROC: 81.07114434242249, Sensitivity/Recall: 81.30081295967102, Precision: 59.8802387714386, F1: 68.96551672683742\n","* (Adam = 0.0001) loss: 0.5654 - tp: 82.0000 - fp: 40.0000 - tn: 136.0000 - fn: 42.0000 - sen: 0.6613 - prc: 0.6721 - auroc: 0.8285 - acc: 0.7267 \n","* (Adam = 0.0001) loss: 0.5766 - tp: 91.0000 - fp: 51.0000 - tn: 125.0000 - fn: 33.0000 - sen: 0.7339 - prc: 0.6408 - auroc: 0.8284 - acc: 0.7200 \n","* (Adam = 0.0001) loss: 0.5749 - tp: 88.0000 - fp: 44.0000 - tn: 132.0000 - fn: 36.0000 - sen: 0.7097 - prc: 0.6667 - auroc: 0.8279 - acc: 0.7333 \n","* (Adam = 0.0001) loss: 0.5751 - tp: 92.0000 - fp: 46.0000 - tn: 130.0000 - fn: 32.0000 - sen: 0.7419 - prc: 0.6667 - auroc: 0.8255 - acc: 0.7400 "]},{"cell_type":"markdown","metadata":{"id":"7lFMSj8pzB1A"},"source":["## (16) simple Transformer model"]},{"cell_type":"code","metadata":{"id":"4-COOidjzEIm"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qOjIXUxzKi4"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1)\n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = tf.keras.layers.GlobalAveragePooling1D()(nl)\n","nl = tf.keras.layers.Dropout(0.25)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_unstruct16_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PG0A6oYtzZn7"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.6336 - tp: 65.0000 - fp: 35.0000 - tn: 141.0000 - fn: 59.0000 - sen: 0.5242 - prc: 0.6500 - auroc: 0.7433 - acc: 0.6867\n","    * Holdout: AUROC: 79.74828481674194, Sensitivity/Recall: 86.9918704032898, Precision: 53.76884341239929, F1: 66.45962685285666\n","* (Adam = 0.0001) loss: 0.6397 - tp: 75.0000 - fp: 48.0000 - tn: 128.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.6098 - auroc: 0.7303 - acc: 0.6767\n","* (Adam = 0.0001) loss: 0.6241 - tp: 67.0000 - fp: 39.0000 - tn: 137.0000 - fn: 57.0000 - sen: 0.5403 - prc: 0.6321 - auroc: 0.7422 - acc: 0.6800 "]},{"cell_type":"markdown","metadata":{"id":"4OgclRodzj0z"},"source":["## (17) Transformer head + TextCNN"]},{"cell_type":"code","metadata":{"id":"ZH9CcIBvzmVQ"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_vAeDNyzo50"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1)\n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","kernels = [3, 5, 7, 10]\n","pooled = []\n","for kernel_size in kernels:\n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl)\n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer)\n","    pooled.append(mini_pooled)\n","nl = tf.keras.layers.Concatenate(axis=1)(pooled)\n","nl = tf.keras.layers.Flatten()(nl)\n","nl = tf.keras.layers.Dropout(0.25)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_unstruct17_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7RjfSUilzvVX"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.8753 - tp: 82.0000 - fp: 32.0000 - tn: 144.0000 - fn: 42.0000 - sen: 0.6613 - prc: 0.7193 - auroc: 0.8130 - acc: 0.7533 \n","* (Adam = 0.0001) loss: 0.8487 - tp: 94.0000 - fp: 55.0000 - tn: 121.0000 - fn: 30.0000 - sen: 0.7581 - prc: 0.6309 - auroc: 0.8160 - acc: 0.7167 \n","* (Adam = 0.0001) loss: 0.8558 - tp: 75.0000 - fp: 28.0000 - tn: 148.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.7282 - auroc: 0.8084 - acc: 0.7433 \n","* (Adam = 0.0001) loss: 0.7989 - tp: 84.0000 - fp: 37.0000 - tn: 139.0000 - fn: 40.0000 - sen: 0.6774 - prc: 0.6942 - auroc: 0.8250 - acc: 0.7433 \n","    * Holdout: AUROC: 80.765700340271, Sensitivity/Recall: 85.36585569381714, Precision: 56.14973306655884, F1: 67.74193644945382"]},{"cell_type":"markdown","metadata":{"id":"4BpMPIHwz5ML"},"source":["## (18) Simple 2x Transformer"]},{"cell_type":"code","metadata":{"id":"p9ACij_Kz78g"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","#total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iRkQNsm1z-OP"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1)\n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = tf.keras.layers.GlobalAveragePooling1D()(nl)\n","nl = tf.keras.layers.Dropout(0.25)(nl)\n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl)\n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","#model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_unstruct18_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmrJ11cS0EWA"},"source":["Result:\n","* (Adam = 0.0001) loss: 0.6378 - tp: 78.0000 - fp: 35.0000 - tn: 141.0000 - fn: 46.0000 - sen: 0.6290 - prc: 0.6903 - auroc: 0.7722 - acc: 0.7300 \n","* (Adam = 0.0001) loss: 0.6502 - tp: 75.0000 - fp: 40.0000 - tn: 136.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.6522 - auroc: 0.7752 - acc: 0.7033\n","    * Holdout:  AUROC: 78.16132307052612,\n","Sensitivity/Recall: 81.30081295967102, Precision: 57.47126340866089, F1: 67.34006666502239\n","* (Adam = 0.0001) loss: 0.6796 - tp: 78.0000 - fp: 49.0000 - tn: 127.0000 - fn: 46.0000 - sen: 0.6290 - prc: 0.6142 - auroc: 0.7558 - acc: 0.6833 "]},{"cell_type":"markdown","metadata":{"id":"q6-YPdoy0Lpr"},"source":["## (19) (15) but with Simple Transformer"]},{"cell_type":"code","metadata":{"id":"rYclIgLc0NVn"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDdueR7U0Qg1"},"source":["input1 = tf.keras.Input(shape=(width,))\n","nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1)\n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = tf.keras.layers.GlobalAveragePooling1D()(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo19_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfvjFq9L0bgh"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5855 - tp: 76.0000 - fp: 37.0000 - tn: 139.0000 - fn: 48.0000 - sen: 0.6129 - prc: 0.6726 - auroc: 0.8179 - acc: 0.7167 \n","* (Adam = 0.0001) loss: 0.5823 - tp: 82.0000 - fp: 36.0000 - tn: 140.0000 - fn: 42.0000 - sen: 0.6613 - prc: 0.6949 - auroc: 0.8215 - acc: 0.7400\n","    * Holdout: AUROC: 82.22635984420776,\n","Sensitivity/Recall: 82.11382031440735, Precision: 60.8433723449707, F1: 69.89619271499818\n","* (Adam = 0.0001) loss: 0.5917 - tp: 85.0000 - fp: 47.0000 - tn: 129.0000 - fn: 39.0000 - sen: 0.6855 - prc: 0.6439 - auroc: 0.8072 - acc: 0.7133\n","* (Adam = 0.0001) loss: 0.5804 - tp: 75.0000 - fp: 28.0000 - tn: 148.0000 - fn: 49.0000 - sen: 0.6048 - prc: 0.7282 - auroc: 0.8176 - acc: 0.7433 "]},{"cell_type":"markdown","metadata":{"id":"mmvOU31i0lJ7"},"source":["## (20) (15) but with Transformer-front TextCNN"]},{"cell_type":"code","metadata":{"id":"dqUGzFuM0nUi"},"source":["#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfddPwdE0qZW"},"source":["input1 = tf.keras.Input(shape=(width,)) \n","nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1) \n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","model.summary()\n","\n","metrics = [\n","    tf.keras.metrics.TruePositives(name='tp'), \n","    tf.keras.metrics.FalsePositives(name='fp'), \n","    tf.keras.metrics.TrueNegatives(name='tn'), \n","    tf.keras.metrics.FalseNegatives(name='fn'), \n","    tf.keras.metrics.Recall(name='sen'), \n","    tf.keras.metrics.Precision(name='prc'), \n","    tf.keras.metrics.AUC(name='auroc'), \n","    tf.keras.metrics.BinaryAccuracy(name='acc')\n","]\n","callbacks=[\n","    tf.keras.callbacks.EarlyStopping(patience=4, verbose=1, restore_best_weights=True, min_delta=0.0001), \n","    tf.keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, verbose=1)\n","]\n","model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)\n","model.fit(train_dataset, epochs=80, validation_data=validation_dataset, verbose=1, callbacks=callbacks)\n","model.evaluate(validation_dataset)\n","model.save_weights('pubmed_combo20_lowva_weights-XXXX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NbMUZ16a0xLk"},"source":["Results:\n","* (Adam = 0.0001) loss: 0.5756 - tp: 81.0000 - fp: 35.0000 - tn: 141.0000 - fn: 43.0000 - sen: 0.6532 - prc: 0.6983 - auroc: 0.8010 - acc: 0.7400 \n","* (Adam = 0.0001) loss: 0.5715 - tp: 82.0000 - fp: 41.0000 - tn: 135.0000 - fn: 42.0000 - sen: 0.6613 - prc: 0.6667 - auroc: 0.8172 - acc: 0.7233 \n","    * Holdout: AUROC: 79.8768937587738, Sensitivity/Recall: 78.86179089546204, Precision: 57.73809552192688, F1: 66.66666766968527\n","* (Adam = 0.0001) loss: 0.6085 - tp: 76.0000 - fp: 34.0000 - tn: 142.0000 - fn: 48.0000 - sen: 0.6129 - prc: 0.6909 - auroc: 0.7843 - acc: 0.7267 "]},{"cell_type":"markdown","metadata":{"id":"mzEykDkYRnOL"},"source":["# Loading Model Weights to Evaluate on Holdout\n","To re-run saved model (assuming you used ```save_weights``` path as described & used above), first re-create the model architecture making sure to sub out ```TokenAndPositionPreTrainedEmbedding``` for ```TokenAndPositionEmbedding```. Then, use ```load_weights``` for the resulting model object to load the prefix (can ignore the ```.index``` and ```.data-00000-of-000001``` extensions). "]},{"cell_type":"code","metadata":{"id":"cshW2ggISxN9"},"source":["# model path\n","model_path = 'pubmed_combo20_lowva_weights-8172'\n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/tf models/\"+model_path+\".data-00000-of-00001\", model_path + \".data-00000-of-00001\")\n","shutil.copy(\"/content/drive/Shared drives/clinicalmodels/tf models/\"+model_path+\".index\", model_path+\".index\")\n","\n","# load appropriate tf.data dataset\n","#total_input = tf.data.Dataset.from_tensor_slices(structuredArray)\n","#total_input = tf.data.Dataset.from_tensor_slices(tokenArray)\n","total_input = tf.data.Dataset.from_tensor_slices((tokenArray, structuredArray))\n","total_output = tf.data.Dataset.from_tensor_slices(outputArray)\n","total_dataset = tf.data.Dataset.zip((total_input, total_output))\n","train_dataset = total_dataset.skip(600).shuffle(1000).batch(15)\n","validation_dataset = total_dataset.skip(300).take(300).batch(15)\n","test_dataset = total_dataset.take(300).batch(15)\n","\n","# set up model -- replace code with original model\n","# (20) (15) but with Transformer-front TextCNN\n","input1 = tf.keras.Input(shape=(width,)) \n","#nl = TokenAndPositionPreTrainedEmbedding(width, vocabulary_size, embedding_dimension, embedding_matrix, trainable = False)(input1) \n","#replace above line \n","nl = TokenAndPositionEmbedding(width, vocabulary_size, embedding_dimension, trainable = False)(input1) \n","nl = TransformerBlock(embedding_dimension, 10, embedding_dimension)(nl)\n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","kernels = [3, 5, 7, 10] \n","pooled = [] \n","for kernel_size in kernels: \n","    mini_layer = tf.keras.layers.Conv1D(256, kernel_size, activation='relu')(nl) \n","    mini_pooled = tf.keras.layers.MaxPooling1D(width - kernel_size + 1)(mini_layer) \n","    pooled.append(mini_pooled) \n","nl = tf.keras.layers.Concatenate(axis=1)(pooled) \n","nl = tf.keras.layers.Flatten()(nl) \n","nl = tf.keras.layers.Dropout(0.50)(nl) \n","nl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","\n","input2 = tf.keras.Input(shape=(structuredArray.shape[1],))\n","sl = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(input2)\n","\n","combo = tf.keras.layers.Concatenate()([nl, sl])\n","nl = combo\n","nl = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl) \n","nl = tf.keras.layers.Dropout(0.5)(nl) \n","nl = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2())(nl)\n","output = tf.keras.layers.Dense(1, activation='sigmoid')(nl)\n","\n","#model = tf.keras.Model(inputs = input1, outputs = output)\n","#model = tf.keras.Model(inputs = input2, outputs = output)\n","model = tf.keras.Model(inputs = (input1, input2), outputs = output)\n","\n","# load the weights\n","model.load_weights(model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"so4rauHfUgqq"},"source":["The model should then be evaluating for decision thresholds every 0.05 to identify the decision threshold which achieves the best F1 score on the validation dataset. "]},{"cell_type":"code","metadata":{"id":"EwOjD7MkS35b"},"source":["thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n","maxf1threshold = 0.0\n","maxf1 = 0.0\n","maxResults = []\n","for threshold in thresholds:\n","    print('\\nThreshold:', threshold)\n","    metrics = [tf.keras.metrics.TruePositives(name='tp', thresholds=threshold), \n","                tf.keras.metrics.FalsePositives(name='fp', thresholds=threshold), \n","                tf.keras.metrics.TrueNegatives(name='tn', thresholds=threshold), \n","                tf.keras.metrics.FalseNegatives(name='fn', thresholds=threshold), \n","                tf.keras.metrics.Recall(name='sen', thresholds=threshold), \n","                tf.keras.metrics.Precision(name='prc', thresholds=threshold), \n","                tf.keras.metrics.AUC(name='auroc', curve='ROC'), \n","                tf.keras.metrics.AUC(name='auprc', curve='PR'), ]\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n","\n","    loss, tp, fp, tn, fn, sens, prec, auroc, auprc = model.evaluate(validation_dataset)\n","    if sens * prec == 0.0:\n","        f1 = 0.0\n","    else:\n","        f1 = 2/(1/sens + 1/prec)\n","\n","    if f1 > maxf1:\n","        maxf1threshold = threshold\n","        maxf1 = f1\n","        maxResults = [tp, fp, tn, fn, sens, prec]\n","\n","print('\\nBest threshold on Test Set:', maxf1threshold)\n","print('Best F1:', maxf1*100)\n","print('True Positives:', maxResults[0])\n","print('False Positives:', maxResults[1])\n","print('True Negatives:', maxResults[2])\n","print('False Negatives:', maxResults[3])\n","print('Sensitivity/Recall:', maxResults[4]*100)\n","print('Precision:', maxResults[5]*100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXwlHcilUmDp"},"source":["Using the decision threshold from the last step, the model is then  evaluated against the holdout set. The predictions from the evaluation are also saved (for use in AUROC / AUPRC curve construction later)."]},{"cell_type":"code","metadata":{"id":"uiTX6EXMS6-J"},"source":["metrics = [tf.keras.metrics.TruePositives(name='tp', thresholds=maxf1threshold), \n","            tf.keras.metrics.FalsePositives(name='fp', thresholds=maxf1threshold), \n","            tf.keras.metrics.TrueNegatives(name='tn', thresholds=maxf1threshold), \n","            tf.keras.metrics.FalseNegatives(name='fn', thresholds=maxf1threshold), \n","            tf.keras.metrics.Recall(name='sens', thresholds=maxf1threshold), \n","            tf.keras.metrics.Precision(name='prec', thresholds=maxf1threshold), \n","            tf.keras.metrics.AUC(name='auroc', curve='ROC'), \n","            tf.keras.metrics.AUC(name='auprc', curve='PR'), ]\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n","loss, tp, fp, tn, fn, sens, prec, auroc, auprc = model.evaluate(test_dataset)\n","print('AUROC:', auroc*100)\n","print('Sensitivity/Recall:', sens*100)\n","print('Precision:', prec*100)\n","if sens * prec == 0.0:\n","    f1 = 0.0\n","    print('F1 not applicable')\n","else:\n","    f1 = 2/(1/sens + 1/prec)\n","    print('F1:', f1*100)\n","print('True Positives', tp)\n","print('False Positives', fp)\n","print('True Negatives', tn)\n","print('False Negatives', fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5OCVhMd7gGgE"},"source":["This model can then be used to generate and save predictions on the test set which can then be used to produce ROC and PRC curves. Note: be sure to remember to only use the line that corresponds to the input expected by your model"]},{"cell_type":"code","metadata":{"id":"qpMGEoUJS9AH"},"source":["#ypred = model.predict(structuredArray[0:300], batch_size=15) # structured model\n","ypred = model.predict(tokenArray[0:300], batch_size=15) # unstructured model\n","#ypred = model.predict([tokenArray[0:300], structuredArray[0:300]], batch_size=15) # combo model\n","yreal = outputArray[0:300]\n","print(ypred.shape, yreal.shape)\n","with open(model_path+'.csv', 'w') as f:\n","    w = csv.writer(f)\n","    for i, j in zip(ypred, yreal):\n","        _ = w.writerow([i[0], j[0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFdNyaYSS-6h"},"source":["shutil.copy(model_path+\".csv\", \"/content/drive/Shared drives/clinicalmodels/tf models/\"+model_path+\".csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IowA_udjUFP0"},"source":[""],"execution_count":null,"outputs":[]}]}