{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import ast\n",
    "import sklearn \n",
    "import math \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('lowva\\lowva.db')\n",
    "conn.text_factory = str\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcohort=pd.read_sql_query('''select pat_deid, lowvadate from outcome''', conn)\n",
    "dfcohort[\"lowvadate\"]=pd.to_datetime(dfcohort[\"lowvadate\"])\n",
    "dfcohort.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric Variables (From Eye Exam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Acuity - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.read_sql_query('''select pat_deid, exam_date, bcvalogmarod, bcvalogmaros from examva\n",
    "''', conn) \n",
    "dfexam.columns = map(str.lower, dfexam.columns)\n",
    "dfexam[\"exam_date\"]=pd.to_datetime(dfexam[\"exam_date\"])\n",
    "dfexam.head()\n",
    "len(dfexam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what we want is a dataframe where for every pat_deid, lowvadate\n",
    "#we have a long list of preindex bcvalogmars for the appropriate eye \n",
    "dfexam=pd.merge(dfexam, dfcohort, on=\"pat_deid\")\n",
    "dfexam=dfexam[dfexam[\"exam_date\"]<=dfexam[\"lowvadate\"]]\n",
    "\n",
    "dfexam.sort_values([\"pat_deid\",\"lowvadate\", \"exam_date\"], inplace=True)\n",
    "#normalize first \n",
    "dfexam[\"bcvalogmarod\"]=(dfexam[\"bcvalogmarod\"]-dfexam[\"bcvalogmarod\"].mean())/dfexam[\"bcvalogmarod\"].std()\n",
    "dfexam[\"bcvalogmaros\"]=(dfexam[\"bcvalogmaros\"]-dfexam[\"bcvalogmaros\"].mean())/dfexam[\"bcvalogmaros\"].std()\n",
    "\n",
    "dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurevariable=\"bcvalogmaros\" #change this depending on which variable we are searching over\n",
    "def gethi(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: hi=max(valuelistnonan)\n",
    "    except: hi = np.nan\n",
    "    return hi \n",
    "\n",
    "def getlo(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: lo=min(valuelistnonan) \n",
    "    except: lo=np.nan \n",
    "    return lo \n",
    "\n",
    "def getmed(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: med = np.percentile(np.array(valuelistnonan), 50)\n",
    "    except: med=np.nan\n",
    "    return med \n",
    "\n",
    "def getrecent(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: recent=valuelistnonan[-1]     \n",
    "    except: recent=np.nan \n",
    "    return recent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gethi(3178,pd.to_datetime(\"2016-11-11\"))\n",
    "getlo(3178,pd.to_datetime(\"2016-11-11\"))\n",
    "getmed(3178,pd.to_datetime(\"2016-11-11\"))\n",
    "getrecent(3178,pd.to_datetime(\"2016-11-11\"))\n",
    "\n",
    "\n",
    "#getlo(1861,pd.to_datetime(\"2016-11-30\"))\n",
    "#getmed(1861,pd.to_datetime(\"2016-12-14\"))\n",
    "#getrecent(4659,pd.to_datetime(\"2016-12-07\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvafeatures=dfcohort[[\"pat_deid\", \"lowvadate\"]]\n",
    "\n",
    "dfvafeatures[\"bcvalogmarodbest\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmarodworst\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmarodmed\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getmed(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmarodlast\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change featurevariable = bcvalogmaros and rerun the function definitions \n",
    "\n",
    "dfvafeatures[\"bcvalogmarosbest\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmarosworst\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmarosmed\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getmed(*x), axis=1)\n",
    "dfvafeatures[\"bcvalogmaroslast\"]=dfvafeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvafeatures.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcohort[dfcohort[\"pat_deid\"]==3178]\n",
    "dfexam[dfexam[\"pat_deid\"]==3178]\n",
    "dfvafeatures[dfvafeatures[\"pat_deid\"]==3178]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOPs - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.read_sql_query('''select * from examiop''', conn) \n",
    "dfexam.columns = map(str.lower, dfexam.columns)\n",
    "dfexam[\"exam_date\"]=pd.to_datetime(dfexam[\"exam_date\"])\n",
    "dfexam.head()\n",
    "len(dfexam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfexam=pd.merge(dfexam, dfcohort, on=\"pat_deid\")\n",
    "dfexam=dfexam[dfexam[\"exam_date\"]<=dfexam[\"lowvadate\"]]\n",
    "\n",
    "dfexam.sort_values([\"pat_deid\",\"lowvadate\", \"exam_date\"], inplace=True   )\n",
    "dfexam=dfexam[dfexam[\"tmethod\"]!=\"null\"]\n",
    "dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmaxt(stringlist): \n",
    "    try: \n",
    "        tlist=ast.literal_eval(stringlist)\n",
    "    except: \n",
    "        return np.nan \n",
    "    numlist=[] \n",
    "    for item in tlist: \n",
    "        try: \n",
    "            itemint=int(item)\n",
    "            numlist.append(itemint)\n",
    "        except: continue  \n",
    "    try: \n",
    "        maxt=max(numlist)\n",
    "    except:\n",
    "        maxt=np.nan\n",
    "    return maxt \n",
    "\n",
    "dfexam[\"todmax\"]=dfexam[\"tod\"].apply(getmaxt)\n",
    "dfexam[\"tosmax\"]=dfexam[\"tos\"].apply(getmaxt)\n",
    "dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize \n",
    "dfexam[\"todmax\"]=(dfexam[\"todmax\"]-dfexam[\"todmax\"].mean())/dfexam[\"todmax\"].std()\n",
    "dfexam[\"tosmax\"]=(dfexam[\"tosmax\"]-dfexam[\"tosmax\"].mean())/dfexam[\"tosmax\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurevariable=\"tosmax\" #change this depending on which variable we are searching over\n",
    "def gethi(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: hi=max(valuelistnonan)\n",
    "    except: hi = np.nan\n",
    "    return hi \n",
    "\n",
    "def getlo(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: lo=min(valuelistnonan) \n",
    "    except: lo=np.nan \n",
    "    return lo \n",
    "\n",
    "def getmed(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: med = np.percentile(np.array(valuelistnonan), 50)\n",
    "    except: med=np.nan\n",
    "    return med \n",
    "\n",
    "def getrecent(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: recent=valuelistnonan[-1]     \n",
    "    except: recent=np.nan \n",
    "    return recent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftfeatures=dfcohort[[\"pat_deid\", \"lowvadate\"]]\n",
    "dftfeatures[\"todlo\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dftfeatures[\"todhi\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n",
    "dftfeatures[\"todmed\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getmed(*x), axis=1)\n",
    "dftfeatures[\"todlast\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)\n",
    "dftfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset featurevariable \n",
    "dftfeatures[\"toslo\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dftfeatures[\"toshi\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n",
    "dftfeatures[\"tosmed\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getmed(*x), axis=1)\n",
    "dftfeatures[\"toslast\"]=dftfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftfeatures.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCT - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.read_sql_query('''select * from examcct''', conn) \n",
    "dfexam.columns = map(str.lower, dfexam.columns)\n",
    "dfexam[\"exam_date\"]=pd.to_datetime(dfexam[\"cctdate\"])\n",
    "del dfexam[\"cctdate\"]\n",
    "#dfexam.head()\n",
    "len(dfexam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.merge(dfexam, dfcohort, on=\"pat_deid\")\n",
    "dfexam=dfexam[dfexam[\"exam_date\"]<=dfexam[\"lowvadate\"]]\n",
    "\n",
    "dfexam.sort_values([\"pat_deid\",\"lowvadate\", \"exam_date\"], inplace=True)\n",
    "\n",
    "#remove outliers - e.g., 26 is clearly a typo as cct is usually a few hundred microns \n",
    "dfexam[\"cctod\"]=np.where(dfexam[\"cctod\"]<300, np.nan, dfexam[\"cctod\"])\n",
    "dfexam[\"cctos\"]=np.where(dfexam[\"cctos\"]<300, np.nan, dfexam[\"cctos\"])\n",
    "\n",
    "#normalize \n",
    "dfexam[\"cctod\"]=(dfexam[\"cctod\"]-dfexam[\"cctod\"].mean())/dfexam[\"cctod\"].std()\n",
    "dfexam[\"cctos\"]=(dfexam[\"cctos\"]-dfexam[\"cctos\"].mean())/dfexam[\"cctos\"].std()\n",
    "\n",
    "\n",
    "dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurevariable=\"cctos\" #change this depending on which variable we are searching over\n",
    "def gethi(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: hi=max(valuelistnonan)\n",
    "    except: hi = np.nan\n",
    "    return hi \n",
    "\n",
    "def getlo(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: lo=min(valuelistnonan) \n",
    "    except: lo=np.nan \n",
    "    return lo \n",
    "\n",
    "def getmed(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: med = np.percentile(np.array(valuelistnonan), 50)\n",
    "    except: med=np.nan\n",
    "    return med \n",
    "\n",
    "def getrecent(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: recent=valuelistnonan[-1]     \n",
    "    except: recent=np.nan \n",
    "    return recent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcctfeatures=dfcohort[[\"pat_deid\", \"lowvadate\"]]\n",
    "dfcctfeatures[\"cctodlast\"]=dfcctfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset feature variable and rerun the functions \n",
    "dfcctfeatures[\"cctoslast\"]=dfcctfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getrecent(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam[dfexam[\"pat_deid\"]==63923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcctfeatures.head(20)\n",
    "len(dfcctfeatures)\n",
    "#many are missing because this just wasnt measured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refraction - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.read_sql_query('''select * from examrx''', conn) \n",
    "dfexam.columns = map(str.lower, dfexam.columns)\n",
    "dfexam[\"exam_date\"]=pd.to_datetime(dfexam[\"exam_date\"])\n",
    "dfexam.head()\n",
    "len(dfexam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this needs to be treated a bit diferently. We want the most myopic spherical equivalent per eye \n",
    "dfexam[\"spheqvod\"]=dfexam[[\"wrxodspheqv\", \"mrxodspheqv\", \"finalrxodspheqv\"]].min(axis=1)\n",
    "dfexam[\"spheqvos\"]=dfexam[[\"wrxosspheqv\", \"mrxosspheqv\", \"finalrxosspheqv\"]].min(axis=1)\n",
    "#dfexam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.merge(dfexam[[\"pat_deid\", \"exam_date\", \"spheqvod\", \"spheqvos\"]], dfcohort, on=\"pat_deid\")\n",
    "dfexam=dfexam[dfexam[\"exam_date\"]<=dfexam[\"lowvadate\"]]\n",
    "\n",
    "dfexam.sort_values([\"pat_deid\",\"lowvadate\", \"exam_date\"], inplace=True)\n",
    "dfexam[\"spheqvod\"]=(dfexam[\"spheqvod\"]-dfexam[\"spheqvod\"].mean())/dfexam[\"spheqvod\"].std()\n",
    "dfexam[\"spheqvos\"]=(dfexam[\"spheqvos\"]-dfexam[\"spheqvos\"].mean())/dfexam[\"spheqvos\"].std()\n",
    "\n",
    "#dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurevariable=\"spheqvos\"\n",
    "def getlo(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    try: lo=min(valuelist) \n",
    "    except: lo=np.nan \n",
    "    return lo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrxfeatures=dfcohort[[\"pat_deid\", \"lowvadate\"]]\n",
    "\n",
    "dfrxfeatures[\"rxodminus\"]=dfrxfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset featurevariable \n",
    "dfrxfeatures[\"rxosminus\"]=dfrxfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrxfeatures.head(20)\n",
    "len(dfrxfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDR - done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.read_sql_query('''select pat_deid, exam_date, feodcdr, feoscdr from examcdr''', conn) \n",
    "dfexam.columns = map(str.lower, dfexam.columns)\n",
    "dfexam[\"exam_date\"]=pd.to_datetime(dfexam[\"exam_date\"])\n",
    "dfexam=dfexam[~((dfexam[\"feodcdr\"].isnull()) & (dfexam[\"feoscdr\"].isnull()))]\n",
    "dfexam.head()\n",
    "len(dfexam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexam=pd.merge(dfexam[[\"pat_deid\", \"exam_date\", \"feodcdr\", \"feoscdr\"]], dfcohort, on=\"pat_deid\")\n",
    "dfexam=dfexam[dfexam[\"exam_date\"]<=dfexam[\"lowvadate\"]]\n",
    "\n",
    "dfexam.sort_values([\"pat_deid\",\"lowvadate\", \"exam_date\"], inplace=True)\n",
    "\n",
    "dfexam[\"feodcdr\"]=(dfexam[\"feodcdr\"]-dfexam[\"feodcdr\"].mean())/dfexam[\"feodcdr\"].std()\n",
    "dfexam[\"feoscdr\"]=(dfexam[\"feoscdr\"]-dfexam[\"feoscdr\"].mean())/dfexam[\"feoscdr\"].std()\n",
    "\n",
    "\n",
    "dfexam.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfexam[\"pat_deid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurevariable=\"feoscdr\" #change this depending on which variable we are searching over\n",
    "def gethi(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: hi=max(valuelistnonan)\n",
    "    except: hi = np.nan\n",
    "    return hi \n",
    "\n",
    "def getlo(pat_deid, lowvadate, value=featurevariable): \n",
    "    valuelist=dfexam[(dfexam[\"pat_deid\"]==pat_deid) & (dfexam[\"lowvadate\"]==lowvadate)][value].tolist() \n",
    "    valuelistnonan=[x for x in valuelist if pd.notnull(x)]\n",
    "    try: lo=min(valuelistnonan) \n",
    "    except: lo=np.nan \n",
    "    return lo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcdrfeatures=dfcohort[[\"pat_deid\", \"lowvadate\"]]\n",
    "dfcdrfeatures[\"cdrodbest\"]=dfcdrfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dfcdrfeatures[\"cdrodworst\"]=dfcdrfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset featurevariable \n",
    "dfcdrfeatures[\"cdrosbest\"]=dfcdrfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: getlo(*x), axis=1)\n",
    "dfcdrfeatures[\"cdrosworst\"]=dfcdrfeatures[[\"pat_deid\", \"lowvadate\"]].apply(lambda x: gethi(*x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcdrfeatures.head(20)\n",
    "len(dfcdrfeatures)\n",
    "#this field has a lot of missing data because not all providers use it - many free-text this finding into their notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfcdrfeatures[~((dfcdrfeatures[\"cdrodbest\"].isnull()) & \n",
    "              (dfcdrfeatures[\"cdrosbest\"].isnull()) & \n",
    "              (dfcdrfeatures[\"cdrodworst\"].isnull()) & \n",
    "              (dfcdrfeatures[\"cdrosworst\"].isnull()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the structured exam features into one matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we are going to get rid of all the redundant lowvadates \n",
    "del dfvafeatures[\"lowvadate\"]\n",
    "del dftfeatures[\"lowvadate\"]\n",
    "del dfcctfeatures[\"lowvadate\"]\n",
    "del dfrxfeatures[\"lowvadate\"]\n",
    "del dfcdrfeatures[\"lowvadate\"]\n",
    "\n",
    "\n",
    "dfexamstructured=pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(dfcohort[[\"pat_deid\"]], dfvafeatures, on=[\"pat_deid\"], how=\"outer\"), \n",
    "         dftfeatures, on=[\"pat_deid\"], how=\"outer\"), \n",
    "        dfcctfeatures, on=[\"pat_deid\"], how=\"outer\"), \n",
    "                  dfrxfeatures, on=[\"pat_deid\"], how=\"outer\"),\n",
    "         dfcdrfeatures, on=[\"pat_deid\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexamstructured.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexamstructured.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coded Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medications - done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn long dataframe to wide and filter out near zero variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeds=pd.read_sql_query('''select * from medslong''', conn) \n",
    "dfmeds.columns = map(str.lower, dfmeds.columns)\n",
    "dfmeds[\"rx_date\"]=pd.to_datetime(dfmeds[\"rx_date\"])\n",
    "#dfmeds.head()\n",
    "len(dfmeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeds=pd.merge(dfmeds,dfcohort[[\"pat_deid\", \"lowvadate\"]], left_on=\"pat_deid\", right_on=\"pat_deid\")\n",
    "dfmeds.sort_values(by=[\"pat_deid\", \"rx_date\"], ascending=True, inplace=True)\n",
    "dfmeds=dfmeds[dfmeds[\"rx_date\"]<=dfmeds[\"lowvadate\"]]\n",
    "#dfmeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmeds[\"pivotvalue\"]=1\n",
    "dfmeds[\"medication_id\"]=dfmeds[\"medication_id\"].astype(int)\n",
    "dfmedswide=dfmeds.pivot_table(values=\"pivotvalue\", index=['pat_deid'], columns='medication_id', fill_value=0)\n",
    "dfmedswide.columns = ['med_'+str(col) for col in dfmedswide.columns.values]\n",
    "dfmedswide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmedswide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's filter out near zero variance features for the medications\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector=VarianceThreshold(.99 * (1 - .99))\n",
    "\n",
    "selector.fit_transform(np.array(dfmedswide.loc[:, 'med_1':'med_590201'])).shape\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "dfmedsfiltered=variance_threshold_selector(dfmedswide.loc[:, 'med_1':'med_590201'], .99 * (1 - .99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmedsfiltered.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmedsfiltered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses - done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdx=pd.read_sql_query('''select * from dxlong''', conn) \n",
    "dfdx.columns = map(str.lower, dfdx.columns)\n",
    "dfdx[\"dx_date\"]=pd.to_datetime(dfdx[\"dx_date\"])\n",
    "dfdx.head()\n",
    "len(dfdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one of the issues is that sometimes icd9 is missing, and sometimes icd10 is missing. Let's create a combined column \n",
    "dfdx[\"icd9_list\"]=\"icd9_\"+dfdx[\"icd9_list\"].astype(str)\n",
    "dfdx[\"icd10_list\"]=\"icd10_\"+dfdx[\"icd10_list\"].astype(str)\n",
    "dfdx[\"icd\"]=np.where(dfdx[\"icd10_list\"]==\"icd10_None\", dfdx['icd9_list'], dfdx[\"icd10_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdxwide=dfdx.pivot_table(values=\"pivotvalue\", index=['pat_deid'], columns='icd', fill_value=0)\n",
    "dfdxwide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=VarianceThreshold(.99 * (1 - .99))\n",
    "\n",
    "selector.fit_transform(np.array(dfdxwide)).shape\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "dfdxfiltered=variance_threshold_selector(dfdxwide, .99 * (1 - .99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfdxfiltered.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surgeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpt=pd.read_sql_query('''select * from cpt''', conn) \n",
    "dfcpt.columns = map(str.lower, dfcpt.columns)\n",
    "dfcpt=pd.merge(dfcohort[\"pat_deid\"], dfcpt, on=\"pat_deid\", how=\"left\").fillna(0)\n",
    "dfcpt.set_index(\"pat_deid\", inplace=True)\n",
    "dfcpt.head()\n",
    "len(dfcpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=VarianceThreshold(.99 * (1 - .99))\n",
    "\n",
    "selector.fit_transform(np.array(dfcpt)).shape\n",
    "\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "dfcptfiltered=variance_threshold_selector(dfcpt, .99 * (1 - .99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcptfiltered.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfpt=pd.read_sql_query('''select d.pat_deid, o.lowvadate - d.birth_date as age, d.gender as gender_Female, d.race as race_asian, d.race as race_white, d.race as race_black, d.race as race_Pacific_Islander,d.race as race_Native_American, d.race as race_other, d.race as race_unknown, d.ethnicity as Ethnicity_Non_Hispanic,d.ethnicity as Ethnicity_Hispanic from demographics as d, outcome as o where d.pat_deid = o.pat_deid\n",
    "''', conn)\n",
    "dfpt.columns = map(str.lower, dfpt.columns)\n",
    "dfpt['gender_female'] = (dfpt['gender_female'] == 'Female').astype(int)\n",
    "dfpt['race_asian'] = (dfpt['race_asian'] == 'Asian').astype(int)\n",
    "dfpt['race_white'] = (dfpt['race_white'] == 'White').astype(int)\n",
    "dfpt['race_black'] = (dfpt['race_black'] == 'Black').astype(int)\n",
    "dfpt['race_pacific_islander'] = (dfpt['race_pacific_islander'] == 'Pacific Islander').astype(int)\n",
    "dfpt['race_native_american'] = (dfpt['race_native_american'] == 'Native_American').astype(int)\n",
    "dfpt['race_other'] = (dfpt['race_other'] == 'Other').astype(int)\n",
    "dfpt['race_unknown'] = (dfpt['race_unknown'] == 'Unknown').astype(int)\n",
    "dfpt['ethnicity_non_hispanic'] = (dfpt['ethnicity_non_hispanic'] == 'Non-Hispanic').astype(int)\n",
    "dfpt['ethnicity_hispanic'] = (dfpt['ethnicity_hispanic'] == 'Hispanic/Latino').astype(int)\n",
    "#dfpt['ethnicity_unkown'] = (dfpt['ethnicity_unknown'] == 'Unknown').astype(int)\n",
    "\n",
    "dfpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfpt[\"birth_date\"]=pd.to_datetime(dfpt[\"birth_date\"])\n",
    "from datetime import timedelta, date\n",
    "future = dfpt['birth_date'] > date(year=2010,month=1,day=1) #specifies the cutoff year\n",
    "dfpt.loc[future, 'birth_date'] -= timedelta(days=365.25*100)\n",
    "dfpt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfpt[\"age\"].mean()\n",
    "dfpt[\"age\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#normalize age\n",
    "dfpt[\"agestandard\"]=(dfpt[\"age\"]-dfpt[\"age\"].mean())/dfpt[\"age\"].std()\n",
    "#check and make sure it worked \n",
    "dfpt[\"agestandard\"].mean()\n",
    "dfpt[\"agestandard\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfpt=pd.read_sql_query('''select demographics.pat_deid, birth_date, gender, race, ethnicity \n",
    "from demographics''',conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfpt[\"raceth\"]=np.where(dfpt[\"ethnicity\"]==\"Hispanic/Latino\", \"Hispanic\", dfpt[\"race\"])\n",
    "dfpt.loc[dfpt.raceth == \"Pacific Islander\", 'raceth'] = \"Asian\"\n",
    "dfpt.loc[dfpt.raceth == \"Native American\", 'raceth'] = \"Other\"\n",
    "dfpt.loc[dfpt.raceth == \"Unknown\", 'raceth'] = \"Other\"\n",
    "\n",
    "dfpt[\"raceth\"].value_counts()\n",
    "dfpt[\"raceth\"].value_counts()/5612\n",
    "dfpt[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfpt=pd.read_sql_query('''select demographics.pat_deid, birth_date, gender, race, ethnicity, lowvadate from demographics, outcome where outcome.pat_deid = demographics.pat_deid''',conn)\n",
    "dfpt.columns = map(str.lower, dfpt.columns)\n",
    "dfpt[\"lowvadate\"]=pd.to_datetime(dfpt[\"lowvadate\"])\n",
    "dfpt.head()\n",
    "dfpt[\"birth_date\"]=pd.to_datetime(dfpt[\"birth_date\"])\n",
    "from datetime import timedelta, date\n",
    "def fix_date(x):\n",
    "    if x.year >=2000:\n",
    "        year = x.year - 100\n",
    "    else:\n",
    "        year = x.year\n",
    "    return date(year,x.month,x.day)\n",
    "\n",
    "dfpt['birth_date'] = dfpt['birth_date'].apply(fix_date)\n",
    "dfpt[\"birth_date\"]=pd.to_datetime(dfpt[\"birth_date\"])\n",
    "dfpt.head()\n",
    "dfpt[\"raceth\"]=np.where(dfpt[\"ethnicity\"]==\"Hispanic/Latino\", \"Hispanic\", dfpt[\"race\"])\n",
    "dfpt[\"raceth\"].value_counts()\n",
    "dfpt.loc[dfpt.raceth == \"Pacific Islander\", 'raceth'] = \"Asian\"\n",
    "dfpt.loc[dfpt.raceth == \"Native American\", 'raceth'] = \"Other\"\n",
    "dfpt.loc[dfpt.raceth == \"Unknown\", 'raceth'] = \"Other\"\n",
    "dfpt['age']=dfpt[\"lowvadate\"].dt.year-dfpt[\"birth_date\"].dt.year\n",
    "dfpt[\"raceth\"].value_counts()\n",
    "dfpt.head()\n",
    "dfpt.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize age\n",
    "dfpt[\"agestandard\"]=(dfpt[\"age\"]-dfpt[\"age\"].mean())/dfpt[\"age\"].std()\n",
    "#check and make sure it worked \n",
    "dfpt[\"agestandard\"].mean()\n",
    "dfpt[\"agestandard\"].std()\n",
    "\n",
    "del dfpt[\"race\"]\n",
    "del dfpt[\"ethnicity\"]\n",
    "\n",
    "dfpt=pd.get_dummies(dfpt)\n",
    "\n",
    "dfpt.head()\n",
    "\n",
    "#clean up original variables by deleting them \n",
    "del dfpt[\"birth_date\"]\n",
    "del dfpt[\"age\"]\n",
    "del dfpt[\"lowvadate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final merge of standardized structured exam features with demographics, and medications,  diagnoses,and surgeries (nzv filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfoutcome=pd.read_sql_query('''select pat_deid, outcome from outcome''', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstructured=pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(pd.merge(dfoutcome, dfpt, on=\"pat_deid\", how=\"outer\"),\n",
    "         dfmedsfiltered, on=\"pat_deid\", how=\"outer\").fillna(0),\n",
    "        dfdxfiltered,on=\"pat_deid\", how=\"outer\").fillna(0),\n",
    "        dfcptfiltered,on=\"pat_deid\", how=\"outer\").fillna(0),\n",
    "        dfvafeatures, on=\"pat_deid\", how=\"outer\"),\n",
    "         dftfeatures, on=\"pat_deid\", how=\"outer\"), \n",
    "        dfcctfeatures, on=\"pat_deid\", how=\"outer\"), \n",
    "        dfrxfeatures, on=\"pat_deid\", how=\"outer\"),\n",
    "         dfcdrfeatures, on=\"pat_deid\", how=\"outer\")\n",
    "\n",
    "#joined the boolean variables first, as they can have fillna(0) without changing the meaning\n",
    "#we will fill/impute the numeric variables but also create a missing values indicator below for those "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def missingindicator(x): \n",
    "    if math.isnan(x): \n",
    "        return 1 \n",
    "    else: \n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingcols=[ 'bcvalogmarodbest',\n",
    " 'bcvalogmarodworst',\n",
    " 'bcvalogmarodmed',\n",
    " 'bcvalogmarodlast',\n",
    " 'bcvalogmarosbest',\n",
    " 'bcvalogmarosworst',\n",
    " 'bcvalogmarosmed',\n",
    " 'bcvalogmaroslast',\n",
    " 'todlo',\n",
    " 'todhi',\n",
    " 'todmed',\n",
    " 'todlast',\n",
    " 'toslo',\n",
    " 'toshi',\n",
    " 'tosmed',\n",
    " 'toslast',\n",
    " 'cctodlast',\n",
    " 'cctoslast',\n",
    " 'rxodminus',\n",
    " 'rxosminus',\n",
    " 'cdrodbest',\n",
    " 'cdrodworst',\n",
    " 'cdrosbest',\n",
    " 'cdrosworst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missingcols: \n",
    "    dfstructured[col+'missing']=dfstructured[col].apply(missingindicator)\n",
    "dfstructured.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now fill the rest of the missing values, equivalent to mean imputation \n",
    "dfstructured=dfstructured.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstructured.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstructured.to_csv('lowva-structured-02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dfstructured.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
